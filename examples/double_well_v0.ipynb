{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62acc182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9315dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import chex\n",
    "import optax\n",
    "from tqdm.autonotebook import tqdm\n",
    "from functools import partial\n",
    "\n",
    "from flow.distribution import make_equivariant_augmented_flow_dist\n",
    "from target import double_well as dw\n",
    "from utils.loggers import ListLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7677202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_64_BIT = False\n",
    "if USE_64_BIT:\n",
    "    from jax.config import config\n",
    "    config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0e925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plotting import plot_history\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25871b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(batch_size, train_test_split_ratio: float = 0.8, seed = 0,\n",
    "                 independant: bool = True, augmented_scale=0.5):\n",
    "    \"\"\"Load dataset and add augmented dataset N(0, 1). \"\"\"\n",
    "    # Make length divisible by batch size also.\n",
    "\n",
    "    dataset = np.load('target/data/dw_data_vertices2_dim2.npy')\n",
    "\n",
    "    if independant:\n",
    "        augmented_dataset = jnp.mean(dataset, axis=(1, 2), keepdims=True) + \\\n",
    "                            jax.random.normal(jax.random.PRNGKey(seed), shape=dataset.shape)*augmented_scale\n",
    "    else:\n",
    "        # p(a, x) = p(x)p(a | x)\n",
    "        augmented_dataset = dataset + jax.random.normal(jax.random.PRNGKey(seed), shape=dataset.shape)*augmented_scale\n",
    "    dataset = jnp.concatenate((dataset, augmented_dataset), axis=-1)\n",
    "\n",
    "\n",
    "    train_index = int(dataset.shape[0] * train_test_split_ratio)\n",
    "    train_set = dataset[:train_index]\n",
    "    test_set = dataset[train_index:]\n",
    "\n",
    "    train_set = train_set[:-(train_set.shape[0] % batch_size)]\n",
    "    test_set = test_set[:-(test_set.shape[0] % batch_size)]\n",
    "    return train_set, test_set\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(2,))\n",
    "def eval(params, x, log_prob_fn,):\n",
    "    log_prob = log_prob_fn.apply(params, x)\n",
    "    info = {\"eval_log_lik\": jnp.mean(log_prob),\n",
    "            \"eval_kl\": jnp.mean(dw.log_prob_fn(x) - log_prob)}\n",
    "    return info\n",
    "\n",
    "\n",
    "def loss_fn(params, x, log_prob_fn):\n",
    "    log_prob = log_prob_fn.apply(params, x)\n",
    "    loss = - jnp.mean(log_prob)\n",
    "    info = {\"loss\": loss}\n",
    "    return loss, info\n",
    "\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(3, 4))\n",
    "def step(params, x, opt_state, log_prob_fn, optimizer):\n",
    "    grad, info = jax.grad(loss_fn, has_aux=True)(params, x, log_prob_fn)\n",
    "    updates, new_opt_state = optimizer.update(grad, opt_state, params=params)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    info.update(grad_norm=optax.global_norm(grad))\n",
    "    return new_params, new_opt_state, info\n",
    "\n",
    "\n",
    "def plot_sample_hist(samples, ax, dim=(0,1)):\n",
    "    d = jnp.linalg.norm(samples[:, 0, dim] - samples[:, 1, dim], axis=-1)\n",
    "    ax.hist(d, bins=50, density=True, alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb704f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = int(2e3)\n",
    "max_global_norm = jnp.inf\n",
    "dim = 2\n",
    "lr = 2e-4\n",
    "n_nodes = 2\n",
    "n_layers = 4\n",
    "batch_size = 256\n",
    "mlp_units = (256, 256)\n",
    "key = jax.random.PRNGKey(0)\n",
    "flow_type = \"nice\"  # \"nice\", \"proj\", \"vector_scale_shift\"\n",
    "identity_init = True # False if flow_type == \"vector_scale_shift\" else True\n",
    "independant_target = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2561fafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_dataset(batch_size, independant=independant_target)\n",
    "\n",
    "# Plot target.\n",
    "fig, axs = plt.subplots(2)\n",
    "plot_sample_hist(train_data, axs[0], dim=(0,1))\n",
    "plot_sample_hist(train_data, axs[1], dim=(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2292c1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = ListLogger()\n",
    "\n",
    "\n",
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def log_prob_fn(x):\n",
    "    distribution = make_equivariant_augmented_flow_dist(\n",
    "        dim=dim, nodes=n_nodes, n_layers=n_layers,\n",
    "        flow_identity_init=identity_init, type=flow_type, mlp_units=mlp_units)\n",
    "    return distribution.log_prob(x)\n",
    "\n",
    "@hk.transform\n",
    "def sample_and_log_prob_fn(sample_shape=()):\n",
    "    distribution = make_equivariant_augmented_flow_dist(\n",
    "        dim=dim, nodes=n_nodes, n_layers=n_layers,\n",
    "        flow_identity_init=identity_init, type=flow_type, mlp_units=mlp_units)\n",
    "    return distribution.sample_and_log_prob(seed=hk.next_rng_key(), sample_shape=sample_shape)\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "params = log_prob_fn.init(rng=subkey, x=jnp.zeros((1, n_nodes, dim*2)))\n",
    "\n",
    "optimizer = optax.chain(optax.zero_nans(), optax.clip_by_global_norm(max_global_norm), optax.adam(lr))\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "\n",
    "\n",
    "def plot(n_samples = 512):\n",
    "    fig, axs = plt.subplots(2)\n",
    "    samples = jax.jit(sample_and_log_prob_fn.apply, static_argnums=(2,))(params, jax.random.PRNGKey(0), (n_samples,))[0]\n",
    "    plot_sample_hist(samples, axs[0], dim=(0,1))\n",
    "    plot_sample_hist(train_data, axs[0], dim=(0,1))\n",
    "    plot_sample_hist(samples, axs[1], dim=(2,3))\n",
    "    plot_sample_hist(train_data, axs[1], dim=(2,3))\n",
    "    plt.show()\n",
    "\n",
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df22456a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(range(n_epoch))\n",
    "for i in pbar:\n",
    "    for x in jnp.reshape(train_data, (-1, batch_size, *train_data.shape[1:])):\n",
    "        params, opt_state, info = step(params, x, opt_state, log_prob_fn, optimizer)\n",
    "        logger.write(info)\n",
    "        if jnp.isnan(info[\"grad_norm\"]):\n",
    "            print(\"nan grad\")\n",
    "            raise Exception(\"nan grad encountered\")\n",
    "\n",
    "    key, subkey = jax.random.split(key)\n",
    "    train_data = jax.random.permutation(subkey, train_data, axis=0)\n",
    "    if i % (n_epoch // 10) == 0:\n",
    "        plot()\n",
    "        eval_info = eval(params, test_data, log_prob_fn)\n",
    "        logger.write(eval_info)\n",
    "\n",
    "\n",
    "plot_history(logger.history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada67787",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c15b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger.history[\"grad_norm\"][-100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cc206d",
   "metadata": {},
   "source": [
    "# Tests\n",
    "Test the distribution and bijector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5099d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.test_utils import test_fn_is_invariant, test_fn_is_equivariant, rotate_translate_2d\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "test_fn_is_invariant(lambda x: log_prob_fn.apply(params, x), subkey, n_nodes=n_nodes)\n",
    "\n",
    "# Check that if we rotated and translate the test set, that the log prob is the same. \n",
    "\n",
    "key1, key2 = jax.random.split(key)\n",
    "theta = jax.random.uniform(key1) * 2*jnp.pi\n",
    "translation = jax.random.normal(key2, shape=(dim,))\n",
    "rotated_test_data = jax.vmap(rotate_translate_2d, in_axes=(0, None, None))(test_data, theta, translation)\n",
    "\n",
    "log_probs_test = log_prob_fn.apply(params, test_data )\n",
    "log_probs_rot_test = log_prob_fn.apply(params, rotated_test_data )\n",
    "\n",
    "# If 64 bit then very small, if 32 bit then still small but less small. \n",
    "print(jnp.sum(jnp.abs(log_probs_test - log_probs_rot_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376b9bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated_test_data[0], test_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9349e03",
   "metadata": {},
   "source": [
    "# More flow layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20a09c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 8\n",
    "lr = 1e-4\n",
    "\n",
    "logger = ListLogger()\n",
    "\n",
    "\n",
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def log_prob_fn(x):\n",
    "    distribution = make_equivariant_augmented_flow_dist(\n",
    "        dim=dim, nodes=n_nodes, n_layers=n_layers,\n",
    "        flow_identity_init=identity_init, type=flow_type, mlp_units=mlp_units)\n",
    "    return distribution.log_prob(x)\n",
    "\n",
    "@hk.transform\n",
    "def sample_and_log_prob_fn(sample_shape=()):\n",
    "    distribution = make_equivariant_augmented_flow_dist(\n",
    "        dim=dim, nodes=n_nodes, n_layers=n_layers,\n",
    "        flow_identity_init=identity_init, type=flow_type, mlp_units=mlp_units)\n",
    "    return distribution.sample_and_log_prob(seed=hk.next_rng_key(), sample_shape=sample_shape)\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "params = log_prob_fn.init(rng=subkey, x=jnp.zeros((1, n_nodes, dim*2)))\n",
    "\n",
    "optimizer = optax.chain(optax.zero_nans(), optax.clip_by_global_norm(max_global_norm), optax.adam(lr))\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "\n",
    "\n",
    "def plot(n_samples = 512):\n",
    "    fig, axs = plt.subplots(2)\n",
    "    samples = jax.jit(sample_and_log_prob_fn.apply, static_argnums=(2,))(params, jax.random.PRNGKey(0), (n_samples,))[0]\n",
    "    plot_sample_hist(samples, axs[0], dim=(0,1))\n",
    "    plot_sample_hist(train_data, axs[0], dim=(0,1))\n",
    "    plot_sample_hist(samples, axs[1], dim=(2,3))\n",
    "    plot_sample_hist(train_data, axs[1], dim=(2,3))\n",
    "    plt.show()\n",
    "\n",
    "plot()\n",
    "\n",
    "\n",
    "\n",
    "pbar = tqdm(range(n_epoch))\n",
    "for i in pbar:\n",
    "    for x in jnp.reshape(train_data, (-1, batch_size, *train_data.shape[1:])):\n",
    "        params, opt_state, info = step(params, x, opt_state, log_prob_fn, optimizer)\n",
    "        logger.write(info)\n",
    "        if jnp.isnan(info[\"grad_norm\"]):\n",
    "            print(\"nan grad\")\n",
    "            raise Exception(\"nan grad encountered\")\n",
    "\n",
    "    key, subkey = jax.random.split(key)\n",
    "    train_data = jax.random.permutation(subkey, train_data, axis=0)\n",
    "    if i % (n_epoch // 10) == 0:\n",
    "        plot()\n",
    "        eval_info = eval(params, test_data, log_prob_fn)\n",
    "        logger.write(eval_info)\n",
    "\n",
    "\n",
    "plot_history(logger.history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c05fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55821244",
   "metadata": {},
   "source": [
    "# Alternative Flow type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1082271e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "flow_type = \"vector_scale_shift\"  # \"nice\", \"proj\", \"vector_scale_shift\"\n",
    "\n",
    "independant_target = True\n",
    "n_layers = 4\n",
    "lr = 1e-4\n",
    "mlp_units = (256, 256)\n",
    "max_global_norm = 40\n",
    "\n",
    "train_data, test_data = load_dataset(batch_size, independant=independant_target)\n",
    "\n",
    "\n",
    "logger = ListLogger()\n",
    "\n",
    "\n",
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def log_prob_fn(x):\n",
    "    distribution = make_equivariant_augmented_flow_dist(\n",
    "        dim=dim, nodes=n_nodes, n_layers=n_layers,\n",
    "        flow_identity_init=identity_init, type=flow_type, mlp_units=mlp_units)\n",
    "    return distribution.log_prob(x)\n",
    "\n",
    "@hk.transform\n",
    "def sample_and_log_prob_fn(sample_shape=()):\n",
    "    distribution = make_equivariant_augmented_flow_dist(\n",
    "        dim=dim, nodes=n_nodes, n_layers=n_layers,\n",
    "        flow_identity_init=identity_init, type=flow_type, mlp_units=mlp_units)\n",
    "    return distribution.sample_and_log_prob(seed=hk.next_rng_key(), sample_shape=sample_shape)\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "params = log_prob_fn.init(rng=subkey, x=jnp.zeros((1, n_nodes, dim*2)))\n",
    "\n",
    "optimizer = optax.chain(optax.zero_nans(), optax.clip_by_global_norm(max_global_norm), optax.adam(lr))\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "\n",
    "\n",
    "def plot(n_samples = 512):\n",
    "    fig, axs = plt.subplots(2)\n",
    "    samples = jax.jit(sample_and_log_prob_fn.apply, static_argnums=(2,))(params, jax.random.PRNGKey(0), (n_samples,))[0]\n",
    "    plot_sample_hist(samples, axs[0], dim=(0,1))\n",
    "    plot_sample_hist(train_data, axs[0], dim=(0,1))\n",
    "    plot_sample_hist(samples, axs[1], dim=(2,3))\n",
    "    plot_sample_hist(train_data, axs[1], dim=(2,3))\n",
    "    plt.show()\n",
    "\n",
    "plot()\n",
    "\n",
    "\n",
    "\n",
    "pbar = tqdm(range(n_epoch))\n",
    "for i in pbar:\n",
    "    for x in jnp.reshape(train_data, (-1, batch_size, *train_data.shape[1:])):\n",
    "        params, opt_state, info = step(params, x, opt_state, log_prob_fn, optimizer)\n",
    "        logger.write(info)\n",
    "        if jnp.isnan(info[\"grad_norm\"]):\n",
    "            print(\"nan grad\")\n",
    "\n",
    "    key, subkey = jax.random.split(key)\n",
    "    train_data = jax.random.permutation(subkey, train_data, axis=0)\n",
    "    if i % (n_epoch // 10) == 0:\n",
    "        plot()\n",
    "        eval_info = eval(params, test_data, log_prob_fn)\n",
    "        logger.write(eval_info)\n",
    "\n",
    "\n",
    "plot_history(logger.history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4604624",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62cc274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test invariance\n",
    "\n",
    "from flow.test_utils import test_fn_is_invariant, test_fn_is_equivariant, rotate_translate_2d\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "test_fn_is_invariant(lambda x: log_prob_fn.apply(params, x), subkey, n_nodes=n_nodes)\n",
    "\n",
    "# Check that if we rotated and translate the test set, that the log prob is the same. \n",
    "\n",
    "key1, key2 = jax.random.split(key)\n",
    "theta = jax.random.uniform(key1) * 2*jnp.pi\n",
    "translation = jax.random.normal(key2, shape=(dim,))\n",
    "rotated_test_data = jax.vmap(rotate_translate_2d, in_axes=(0, None, None))(test_data, theta, translation)\n",
    "\n",
    "log_probs_test = log_prob_fn.apply(params, test_data )\n",
    "log_probs_rot_test = log_prob_fn.apply(params, rotated_test_data )\n",
    "\n",
    "# If 64 bit then very small, if 32 bit then still small but less small. \n",
    "print(jnp.sum(jnp.abs(log_probs_test - log_probs_rot_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0524151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check forward and inverse of the model are the same for log prob. \n",
    "\n",
    "sample, log_prob = sample_and_log_prob_fn.apply(params, jax.random.PRNGKey(0), (10,))\n",
    "\n",
    "log_prob_check = log_prob_fn.apply(params, sample)\n",
    "\n",
    "log_prob - log_prob_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fdf5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331bca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c544dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
